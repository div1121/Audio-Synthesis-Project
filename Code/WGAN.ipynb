{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgV0DvH6H2w5"
      },
      "source": [
        "!pip install tensorflow_io\n",
        "!pip install pyyaml h5py  # Required to save models in HDF5 format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl0A0jtrjT0j"
      },
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas\n",
        "import tensorflow_datasets as tfds\n",
        "import time\n",
        "import librosa.display as lidp\n",
        "import tensorflow_io as tfio\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import Conv1DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.constraints import Constraint\n",
        "from numpy import expand_dims\n",
        "from numpy import mean\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from IPython import display\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc48sm1YAiZZ"
      },
      "source": [
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZL-YAfCjW3p"
      },
      "source": [
        "data, info = tfds.load('nsynth', try_gcs=True, split='train', with_info=True)\n",
        "assert isinstance(data, tf.data.Dataset)\n",
        "dataset = data.shuffle(1024).batch(32)\n",
        "total = len(dataset)\n",
        "print(len(dataset))\n",
        "data = data.shuffle(1024).batch(32).repeat()\n",
        "db_iter = iter(data)\n",
        "#get data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH0S4-ySj4CE"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#login your dirve so that you can save model data for later training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk8IqDY1onte"
      },
      "source": [
        "class ClipConstraint(Constraint):\n",
        "\t# set clip value when initialized\n",
        "\tdef __init__(self, clip_value):\n",
        "\t\tself.clip_value = clip_value\n",
        " \n",
        "\t# clip model weights to hypercube\n",
        "\tdef __call__(self, weights):\n",
        "\t\treturn backend.clip(weights, -self.clip_value, self.clip_value)\n",
        " \n",
        "\t# get the config\n",
        "\tdef get_config(self):\n",
        "\t\treturn {'clip_value': self.clip_value}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTpT_HbqoE7r"
      },
      "source": [
        "def wasserstein_loss(y_true, y_pred):\n",
        "\treturn backend.mean(y_true * y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkylFmQDyCzX"
      },
      "source": [
        "def define_critic(d,c,in_shape=(16384,1)):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# weight constraint\n",
        "\tconst = ClipConstraint(0.01)\n",
        " \n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\n",
        "\t# downsample\n",
        "\tmodel.add(Conv1D(d, 25, strides=4, padding='same', kernel_initializer=init, kernel_constraint=const, input_shape=in_shape))\n",
        "\tassert model.output_shape == (None, 4096, d)\n",
        "\t# model.add(BatchNormalization())\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\t# downsample\n",
        "\tmodel.add(Conv1D(2*d, 25, strides=4, padding='same', kernel_initializer=init, kernel_constraint=const))\n",
        "\tassert model.output_shape == (None, 1024, 2*d)\n",
        "\t# model.add(BatchNormalization())\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\t# downsample\n",
        "\tmodel.add(Conv1D(4*d, 25, strides=4, padding='same', kernel_initializer=init, kernel_constraint=const))\n",
        "\tassert model.output_shape == (None, 256, 4*d)\n",
        "\t# model.add(BatchNormalization())\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\t# downsample\n",
        "\tmodel.add(Conv1D(8*d, 25, strides=4, padding='same', kernel_initializer=init, kernel_constraint=const))\n",
        "\tassert model.output_shape == (None, 64, 8*d)\n",
        "\t# model.add(BatchNormalization())\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\t# downsample\n",
        "\tmodel.add(Conv1D(16*d, 25, strides=4, padding='same', kernel_initializer=init, kernel_constraint=const))\n",
        "\tassert model.output_shape == (None, 16, 16*d)\n",
        "\t# model.add(BatchNormalization())\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\t# scoring, linear activation\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(1))\n",
        "\n",
        "\t# compile model\n",
        "\topt = RMSprop(lr=0.00005)\n",
        "\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "\tprint(model.summary())\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6-wYx3H0UpQ"
      },
      "source": [
        "def define_generator(latent_dim,d,c):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\n",
        "\t# foundation\n",
        "\tn_nodes = 256 * d\n",
        "\tmodel.add(Dense(n_nodes, kernel_initializer=init, input_dim=latent_dim))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Reshape((16, 16*d)))\n",
        "\tassert model.output_shape == (None, 16, 16*d)\n",
        "\n",
        "\t# upsample\n",
        "\tmodel.add(Conv1DTranspose(8*d, 25, strides=4, padding='same', kernel_initializer=init))\n",
        "\tassert model.output_shape == (None, 64, 8*d)\n",
        "\t#model.add(BatchNormalization())\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\t# upsample\n",
        "\tmodel.add(Conv1DTranspose(4*d, 25, strides=4, padding='same', kernel_initializer=init))\n",
        "\tassert model.output_shape == (None, 256, 4*d)\n",
        "\t#model.add(BatchNormalization())\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\t# upsample\n",
        "\tmodel.add(Conv1DTranspose(2*d, 25, strides=4, padding='same', kernel_initializer=init))\n",
        "\tassert model.output_shape == (None, 1024, 2*d)\n",
        "\t#model.add(BatchNormalization())\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\t# upsample\n",
        "\tmodel.add(Conv1DTranspose(d, 25, strides=4, padding='same', kernel_initializer=init))\n",
        "\tassert model.output_shape == (None, 4096, d)\n",
        "\t#model.add(BatchNormalization())\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\t# output\n",
        "\tmodel.add(Conv1DTranspose(c, 25, strides=4, activation='tanh', padding='same', kernel_initializer=init))\n",
        "\tassert model.output_shape == (None, 16384, c)\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlBvG-B16Sk2"
      },
      "source": [
        "def define_gan(generator, critic):\n",
        "\t# make weights in the critic not trainable\n",
        "\tcritic.trainable = False\n",
        "\t# connect them\n",
        "\tmodel = Sequential()\n",
        "\t# add generator\n",
        "\tmodel.add(generator)\n",
        "\t# add the critic\n",
        "\tmodel.add(critic)\n",
        "\t# compile model\n",
        "\topt = RMSprop(lr=0.00005)\n",
        "\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "\tprint(model.summary())\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6usMUmDL2zAi"
      },
      "source": [
        "def generate_real_samples(dataset, n_samples):\n",
        "\t# choose random instances\n",
        "\tbatch = next(dataset)\n",
        "\tsound = batch[\"audio\"]\n",
        "\tsound = sound[:,0:16384]\n",
        "\tX = tf.reshape(sound,[sound.shape[0],16384,1])\n",
        "\t# generate class labels, -1 for 'real'\n",
        "\ty = -ones((sound.shape[0], 1))\n",
        "\treturn X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_JUAWzs25yV"
      },
      "source": [
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95XjoVkj3JjL"
      },
      "source": [
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "\t# generate points in latent space\n",
        "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\tX = generator.predict(x_input)\n",
        "\t# create class labels with 1.0 for 'fake'\n",
        "\ty = ones((n_samples, 1))\n",
        "\treturn X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABUtIIEe3TzZ"
      },
      "source": [
        "def summarize_performance(step, g_model, gan_model, c_model, latent_dim, n_samples=16):\n",
        "  # filename2 = '/content/gdrive/My Drive/WGAN/gmodel_%04d.h5' % (step+1)\n",
        "  # g_model.save(filename2)\n",
        "\n",
        "  # filename3 = '/content/gdrive/My Drive/WGAN/cmodel_%04d.h5' % (step+1)\n",
        "  # c_model.save(filename3)\n",
        "\n",
        "  # filename4 = '/content/gdrive/My Drive/WGAN/ganmodel_%04d.h5' % (step+1)\n",
        "  # gan_model.save(filename4)\n",
        "  \n",
        "  checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "  \n",
        "  # prepare fake examples\n",
        "  X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "  fig,axes = plt.subplots(16,1,figsize=(15, 30))\n",
        "  for i in range(16):\n",
        "    x = np.linspace(0,16384,16384)\n",
        "    k = np.reshape(X[i,:,0],(16384))\n",
        "    display.display(display.Audio(k, rate=16000))\n",
        "    axes[i].plot(x,k)\n",
        "  filename1 = '/content/gdrive/My Drive/WGAN/sound_at_epoch_%04d.png' % (step+1)\n",
        "  plt.savefig(filename1)\n",
        "  plt.show()\n",
        "\n",
        "  print('>Saved: %s' % (filename1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DaZ-j_Z51aB"
      },
      "source": [
        "def plot_history(d1_hist, d2_hist, g_hist, count=-1):\n",
        "\t# plot history\n",
        "\tplt.plot(d1_hist, label='crit_real')\n",
        "\tplt.plot(d2_hist, label='crit_fake')\n",
        "\tplt.plot(g_hist, label='gen')\n",
        "\tplt.legend()\n",
        "\tname = '/content/gdrive/My Drive/WGAN/plot_line_plot_loss_%04d.png' % count\n",
        "\tplt.savefig(name)\n",
        "\tplt.show()\n",
        "\tplt.close()\n",
        "\twith open(\"/content/gdrive/My Drive/WGAN/d1hist.txt\", \"w\") as fp:\n",
        "\t\tjson.dump(d1_hist, fp)\n",
        "\twith open(\"/content/gdrive/My Drive/WGAN/d2hist.txt\", \"w\") as fpa:\n",
        "\t\tjson.dump(d2_hist, fpa)\n",
        "\twith open(\"/content/gdrive/My Drive/WGAN/ghist.txt\", \"w\") as fpb:\n",
        "\t\tjson.dump(g_hist, fpb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPRM9GiGhTR9"
      },
      "source": [
        "c1_hist, c2_hist, g_hist = list(), list(), list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2ovBhLRA_w2"
      },
      "source": [
        "with open(\"/content/gdrive/My Drive/WGAN/d1hist.txt\", \"r\") as fp:\n",
        "  c1_hist = json.load(fp)\n",
        "with open(\"/content/gdrive/My Drive/WGAN/d2hist.txt\", \"r\") as fp:\n",
        "\tc2_hist = json.load(fp)\n",
        "with open(\"/content/gdrive/My Drive/WGAN/ghist.txt\", \"r\") as fp:\n",
        "\tg_hist = json.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEmrT5Lh6JmF"
      },
      "source": [
        "# train the generator and critic\n",
        "def train(g_model, c_model, gan_model, dataset, latent_dim, n_epochs=5, n_batch=64, n_critic=5):\n",
        "\t# calculate the number of batches per training epoch\n",
        "\tbat_per_epo = total\n",
        "\t# calculate the number of training iterations\n",
        "\tn_steps = bat_per_epo * n_epochs\n",
        "\t# calculate the size of half a batch of samples\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\t# lists for keeping track of loss\n",
        "\t# c1_hist, c2_hist, g_hist = list(), list(), list()\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_steps):\n",
        "\t\t# update the critic more than the generator\n",
        "\t\tc1_tmp, c2_tmp = list(), list()\n",
        "\t\tfor _ in range(n_critic):\n",
        "\t\t\t# get randomly selected 'real' samples\n",
        "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t\t# update critic model weights\n",
        "\t\t\tc_loss1 = c_model.train_on_batch(X_real, y_real)\n",
        "\t\t\tc1_tmp.append(c_loss1)\n",
        "\t\t\t# generate 'fake' examples\n",
        "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "\t\t\t# update critic model weights\n",
        "\t\t\tc_loss2 = c_model.train_on_batch(X_fake, y_fake)\n",
        "\t\t\tc2_tmp.append(c_loss2)\t\t\t\n",
        "\t\t# store critic loss\n",
        "\t\tc1_hist.append(mean(c1_tmp))\n",
        "\t\tc2_hist.append(mean(c2_tmp))\n",
        "\t\t# prepare points in latent space as input for the generator\n",
        "\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t# create inverted labels for the fake samples\n",
        "\t\ty_gan = -ones((n_batch, 1))\n",
        "\t\t# update the generator via the critic's error\n",
        "\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\t\tg_hist.append(g_loss)\n",
        "\t\t# summarize loss on this batch\n",
        "\t\tprint('>%d, c1=%.3f, c2=%.3f g=%.3f' % (i+1, c1_hist[-1], c2_hist[-1], g_loss))\n",
        "\t\t# evaluate the model performance every 'epoch'\n",
        "\t\tif (i+1) % (bat_per_epo//3) == 0:\n",
        "\t\t\tsummarize_performance(len(c1_hist), g_model, gan_model, c_model, latent_dim)\n",
        "\t\t\tplot_history(c1_hist, c2_hist, g_hist, len(c1_hist))\n",
        "\t# line plots of loss\n",
        "\tplot_history(c1_hist, c2_hist, g_hist,len(c1_hist))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRXP8gWgBHld"
      },
      "source": [
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "# create the critic\n",
        "critic = keras.models.load_model('/content/gdrive/My Drive/WGAN/cmodel_33132.h5',custom_objects={'ClipConstraint': ClipConstraint,'wasserstein_loss': wasserstein_loss})\n",
        "# create the generator\n",
        "generator = keras.models.load_model('/content/gdrive/My Drive/WGAN/gmodel_33132.h5')\n",
        "# create the gan\n",
        "gan_model = keras.models.load_model('/content/gdrive/My Drive/WGAN/ganmodel_33132.h5',custom_objects={'ClipConstraint': ClipConstraint,'wasserstein_loss': wasserstein_loss})\n",
        "\n",
        "print(critic.summary())\n",
        "print(gan_model.summary())\n",
        "# print(critic.optimizer)\n",
        "\n",
        "# train model\n",
        "# train(generator, critic, gan_model, db_iter, latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0Fjkgyek1lY"
      },
      "source": [
        "generator = gan_model.get_layer(\"sequential_4\")\n",
        "print(generator)\n",
        "print(gan_model.get_layer(\"sequential_4\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHKqMYGSkRME"
      },
      "source": [
        "step = -3\n",
        "X, _ = generate_fake_samples(generator, latent_dim, 16)\n",
        "fig,axes = plt.subplots(16,1,figsize=(15, 30))\n",
        "for i in range(16):\n",
        "  x = np.linspace(0,16384,16384)\n",
        "  k = np.reshape(X[i,:,0],(16384))\n",
        "  display.display(display.Audio(k, rate=16000))\n",
        "  axes[i].plot(x,k)\n",
        "filename1 = '/content/gdrive/My Drive/WGAN/sound_at_epoch_%04d.png' % (step+1)\n",
        "plt.savefig(filename1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ-2yh-KnB9u"
      },
      "source": [
        "train(generator, critic, gan_model, db_iter, latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQrH1nDt7WyU"
      },
      "source": [
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "# create the critic\n",
        "critic = define_critic(64,1)\n",
        "# create the generator\n",
        "generator = define_generator(latent_dim,64,1)\n",
        "# create the gan\n",
        "gan_model = define_gan(generator, critic)\n",
        "# train model\n",
        "# train(generator, critic, gan_model, db_iter, latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgmV6ze8d48_"
      },
      "source": [
        "checkpoint_dir = '/content/gdrive/My Drive/WGAN'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator=generator,\n",
        "                  critic=critic,\n",
        "                  gan_model=gan_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWNLIAPahscc"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LOgmpYXhkQJ"
      },
      "source": [
        "checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Aa1bODSzNkH"
      },
      "source": [
        "generator.save(\"/content/gdrive/My Drive/WGAN/bmodel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNPbhmGezWR6"
      },
      "source": [
        "model = keras.models.load_model(\"/content/gdrive/My Drive/WGAN/bmodel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQb7qD_57XRk"
      },
      "source": [
        "train(generator, critic, gan_model, db_iter, latent_dim, n_epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ONz4A8BgUOD"
      },
      "source": [
        "summarize_performance(9038,generator,critic,100,16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLrIa9jAibz4"
      },
      "source": [
        "plot_history(c1_hist,c2_hist,g_hist,1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}